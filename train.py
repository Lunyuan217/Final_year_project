# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dGXTh7Mw5M4peNh5zqbXcWUCZjciYt5q
"""

import warnings
import torch
import numpy as np
from utils import PSNR_cal
from kornia import rotate
from random import sample
from network_arch import UNet
import os
def EI_train_loop(dataloader, model, loss_fn, optimizer,radon,alpha,trans):
    model.train()
    train_loss, train_psnr=[], []
    for batch,X in enumerate(dataloader):
        X = X.cuda()
        y = radon.forward(X)    
        filtered_sinogram = radon.filter_sinogram(y)
        x1 = model(radon.backprojection(filtered_sinogram))  # calculate the FBP
        rotate_angles = np.linspace(1, 360, 360, endpoint=True)
  
        if trans == 1 :

          angles=sample(list(rotate_angles),1)
          x2_1 = rotate(x1, torch.Tensor([angles[0]]).cuda())
          # x2 = torch.stack([x2_1,x2_2,x2_3,x2_4,x2_5],dim=0)
          sinogram1 = radon.forward(x2_1)
          x3_1 = model(radon.backprojection(radon.filter_sinogram(sinogram1)))
          loss_trans=loss_fn(x3_1,x2_1)
          # sinogram = radon.forward(x2)
          # x3 = model(radon.backprojection(radon.filter_sinogram(sinogram)))
          # loss_trans=loss_fn(x3,x2)
          loss = loss_fn(radon.forward(x1), y)+alpha*loss_trans
          train_loss.append(loss.item())
          train_psnr.append(PSNR_cal(X, x1,torch.max(x1)))
          # Backpropagation
          optimizer.zero_grad()
          loss.backward()
          optimizer.step()
        
        if trans == 3 :

          angles=sample(list(rotate_angles),3)
          x2_1 = rotate(x1, torch.Tensor([angles[0]]).cuda())
          x2_2 = rotate(x1, torch.Tensor([angles[1]]).cuda())
          x2_3 = rotate(x1, torch.Tensor([angles[2]]).cuda())
          
          # x2 = torch.stack([x2_1,x2_2,x2_3,x2_4,x2_5],dim=0)
          sinogram1 = radon.forward(x2_1)
          sinogram2 = radon.forward(x2_2)
          sinogram3 = radon.forward(x2_3)
          
          x3_1 = model(radon.backprojection(radon.filter_sinogram(sinogram1)))
          x3_2 = model(radon.backprojection(radon.filter_sinogram(sinogram2)))
          x3_3 = model(radon.backprojection(radon.filter_sinogram(sinogram3)))
          
          loss_trans=(loss_fn(x3_1,x2_1)+loss_fn(x3_2,x2_2)+loss_fn(x3_3,x2_3))/3
          # sinogram = radon.forward(x2)
          # x3 = model(radon.backprojection(radon.filter_sinogram(sinogram)))
          # loss_trans=loss_fn(x3,x2)
          loss = loss_fn(radon.forward(x1), y)+alpha*loss_trans
          train_loss.append(loss.item())
          train_psnr.append(PSNR_cal(X, x1,torch.max(x1)))
          # Backpropagation
          optimizer.zero_grad()
          loss.backward()
          optimizer.step()
        
        if trans == 5 :

          angles=sample(list(rotate_angles),5)
          x2_1 = rotate(x1, torch.Tensor([angles[0]]).cuda())
          x2_2 = rotate(x1, torch.Tensor([angles[1]]).cuda())
          x2_3 = rotate(x1, torch.Tensor([angles[2]]).cuda())
          x2_4 = rotate(x1, torch.Tensor([angles[3]]).cuda())
          x2_5 = rotate(x1, torch.Tensor([angles[4]]).cuda())
          # x2 = torch.stack([x2_1,x2_2,x2_3,x2_4,x2_5],dim=0)
          sinogram1 = radon.forward(x2_1)
          sinogram2 = radon.forward(x2_2)
          sinogram3 = radon.forward(x2_3)
          sinogram4 = radon.forward(x2_4)
          sinogram5 = radon.forward(x2_5)
          x3_1 = model(radon.backprojection(radon.filter_sinogram(sinogram1)))
          x3_2 = model(radon.backprojection(radon.filter_sinogram(sinogram2)))
          x3_3 = model(radon.backprojection(radon.filter_sinogram(sinogram3)))
          x3_4 = model(radon.backprojection(radon.filter_sinogram(sinogram4)))
          x3_5 = model(radon.backprojection(radon.filter_sinogram(sinogram5)))
          loss_trans=(loss_fn(x3_1,x2_1)+loss_fn(x3_2,x2_2)+loss_fn(x3_3,x2_3)+loss_fn(x3_4,x2_4)+loss_fn(x3_5,x2_5))/5
          # sinogram = radon.forward(x2)
          # x3 = model(radon.backprojection(radon.filter_sinogram(sinogram)))
          # loss_trans=loss_fn(x3,x2)
          loss = loss_fn(radon.forward(x1), y)+alpha*loss_trans
          train_loss.append(loss.item())
          train_psnr.append(PSNR_cal(X, x1,torch.max(x1)))
          # Backpropagation
          optimizer.zero_grad()
          loss.backward()
          optimizer.step()

        if trans == 7 :

          angles=sample(list(rotate_angles),7)
          x2_1 = rotate(x1, torch.Tensor([angles[0]]).cuda())
          x2_2 = rotate(x1, torch.Tensor([angles[1]]).cuda())
          x2_3 = rotate(x1, torch.Tensor([angles[2]]).cuda())
          x2_4 = rotate(x1, torch.Tensor([angles[3]]).cuda())
          x2_5 = rotate(x1, torch.Tensor([angles[4]]).cuda())
          x2_6 = rotate(x1, torch.Tensor([angles[5]]).cuda())
          x2_7 = rotate(x1, torch.Tensor([angles[6]]).cuda())
          
          # x2 = torch.stack([x2_1,x2_2,x2_3,x2_4,x2_5],dim=0)
          sinogram1 = radon.forward(x2_1)
          sinogram2 = radon.forward(x2_2)
          sinogram3 = radon.forward(x2_3)
          sinogram4 = radon.forward(x2_4)
          sinogram5 = radon.forward(x2_5)
          sinogram6 = radon.forward(x2_6)
          sinogram7 = radon.forward(x2_7)
          
          x3_1 = model(radon.backprojection(radon.filter_sinogram(sinogram1)))
          x3_2 = model(radon.backprojection(radon.filter_sinogram(sinogram2)))
          x3_3 = model(radon.backprojection(radon.filter_sinogram(sinogram3)))
          x3_4 = model(radon.backprojection(radon.filter_sinogram(sinogram4)))
          x3_5 = model(radon.backprojection(radon.filter_sinogram(sinogram5)))
          x3_6 = model(radon.backprojection(radon.filter_sinogram(sinogram6)))
          x3_7 = model(radon.backprojection(radon.filter_sinogram(sinogram7)))
          
          loss_trans=(loss_fn(x3_1,x2_1)+loss_fn(x3_2,x2_2)+loss_fn(x3_3,x2_3)+loss_fn(x3_4,x2_4)+loss_fn(x3_5,x2_5)+loss_fn(x3_6,x2_6)+loss_fn(x3_7,x2_7))/7
          # sinogram = radon.forward(x2)
          # x3 = model(radon.backprojection(radon.filter_sinogram(sinogram)))
          # loss_trans=loss_fn(x3,x2)
          loss = loss_fn(radon.forward(x1), y)+alpha*loss_trans
          train_loss.append(loss.item())
          train_psnr.append(PSNR_cal(X, x1,torch.max(x1)))
          # Backpropagation
          optimizer.zero_grad()
          loss.backward()
          optimizer.step()
        
        if trans == 10 :

          angles=sample(list(rotate_angles),10)
          x2_1 = rotate(x1, torch.Tensor([angles[0]]).cuda())
          x2_2 = rotate(x1, torch.Tensor([angles[1]]).cuda())
          x2_3 = rotate(x1, torch.Tensor([angles[2]]).cuda())
          x2_4 = rotate(x1, torch.Tensor([angles[3]]).cuda())
          x2_5 = rotate(x1, torch.Tensor([angles[4]]).cuda())
          x2_6 = rotate(x1, torch.Tensor([angles[5]]).cuda())
          x2_7 = rotate(x1, torch.Tensor([angles[6]]).cuda())
          x2_8 = rotate(x1, torch.Tensor([angles[7]]).cuda())
          x2_9 = rotate(x1, torch.Tensor([angles[8]]).cuda())
          x2_10 = rotate(x1, torch.Tensor([angles[9]]).cuda())
          # x2 = torch.stack([x2_1,x2_2,x2_3,x2_4,x2_5],dim=0)
          sinogram1 = radon.forward(x2_1)
          sinogram2 = radon.forward(x2_2)
          sinogram3 = radon.forward(x2_3)
          sinogram4 = radon.forward(x2_4)
          sinogram5 = radon.forward(x2_5)
          sinogram6 = radon.forward(x2_6)
          sinogram7 = radon.forward(x2_7)
          sinogram8 = radon.forward(x2_8)
          sinogram9 = radon.forward(x2_9)
          sinogram10 = radon.forward(x2_10)
          x3_1 = model(radon.backprojection(radon.filter_sinogram(sinogram1)))
          x3_2 = model(radon.backprojection(radon.filter_sinogram(sinogram2)))
          x3_3 = model(radon.backprojection(radon.filter_sinogram(sinogram3)))
          x3_4 = model(radon.backprojection(radon.filter_sinogram(sinogram4)))
          x3_5 = model(radon.backprojection(radon.filter_sinogram(sinogram5)))
          x3_6 = model(radon.backprojection(radon.filter_sinogram(sinogram6)))
          x3_7 = model(radon.backprojection(radon.filter_sinogram(sinogram7)))
          x3_8 = model(radon.backprojection(radon.filter_sinogram(sinogram8)))
          x3_9 = model(radon.backprojection(radon.filter_sinogram(sinogram9)))
          x3_10 = model(radon.backprojection(radon.filter_sinogram(sinogram10)))
          loss_trans=(loss_fn(x3_1,x2_1)+loss_fn(x3_2,x2_2)+loss_fn(x3_3,x2_3)+loss_fn(x3_4,x2_4)+loss_fn(x3_5,x2_5)+loss_fn(x3_6,x2_6)+loss_fn(x3_7,x2_7)+loss_fn(x3_8,x2_8)+loss_fn(x3_9,x2_9)+loss_fn(x3_10,x2_10))/10
          # sinogram = radon.forward(x2)
          # x3 = model(radon.backprojection(radon.filter_sinogram(sinogram)))
          # loss_trans=loss_fn(x3,x2)
          loss = loss_fn(radon.forward(x1), y)+alpha*loss_trans
          train_loss.append(loss.item())
          train_psnr.append(PSNR_cal(X, x1,torch.max(x1)))
          # Backpropagation
          optimizer.zero_grad()
          loss.backward()
          optimizer.step()


    print(f"Train : \n  Avg loss: {np.mean(train_loss):>8f} \n")
    print(f"  Avg psnr: {np.mean(train_psnr):>8f} \n")
    return np.mean(train_psnr),np.mean(train_loss)


def Supervised_train_loop(dataloader, model, loss_fn, optimizer, radon):
    model.train()
    train_loss, train_psnr=[], []
    for batch,X in enumerate(dataloader):
        X = X.cuda()
        y = radon.forward(X)    
        filtered_sinogram = radon.filter_sinogram(y)
        fbpy = radon.backprojection(filtered_sinogram)  # calculate the FBP
        pred = model(fbpy.float())
        loss = loss_fn(pred, X)
        train_loss.append(loss.item())
        train_psnr.append(PSNR_cal(X, pred,torch.max(pred)))
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    print(f"Train : \n  Avg loss: {np.mean(train_loss):>8f} \n")
    print(f"  Avg psnr: {np.mean(train_psnr):>8f} \n")
    return np.mean(train_psnr),np.mean(train_loss)

def EI_train(train_dataloader,model,path,loss_fn,optimizer,radon,alpha,epochs,scheduler,trans):
    warnings.filterwarnings("ignore",category=DeprecationWarning)
    epoch_psnr_train,epoch_psnr_loss = 0,0
    lists = os.listdir(path)  # list all the files and store to lists
    if lists:                                    # if there is a check points, start training from the newest check point
      lists.sort(key=lambda fn: os.path.getmtime(path + "/" + fn))  # arrange by time 
      file_new = os.path.join(path, lists[-1])                     # get the newest file
      ckpt = torch.load(file_new)
      model.load_state_dict(ckpt['net_state_dict'])
      epoch = ckpt['epoch']
      for e in range(epoch,epochs):
        print(f"Epoch {e+1}\n-------------------------------")
        epoch_psnr_train,epoch_psnr_loss = EI_train_loop(train_dataloader, model, loss_fn, optimizer,radon,alpha,trans)
        scheduler.step()
        if path is not None and (e + 1) % 100 == 0 :
                model.eval()
                ckpt = {
                    'epoch': e + 1,
                    'epoch_psnr_train': epoch_psnr_train,
                    'epoch_psnr_loss' : epoch_psnr_loss,
                    'net_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                }
                torch.save(ckpt, os.path.join(path, 'ckp_epoch_{}.pt'.format(e+1)))
    else:                                      # if there is no check point, start from 0
      for e in range(epochs):
        print(f"Epoch {e+1}\n-------------------------------")
        epoch_psnr_train,epoch_psnr_loss = EI_train_loop(train_dataloader, model, loss_fn, optimizer,radon,alpha,trans)
        scheduler.step()
        if path is not None and (e + 1) % 100 == 0 :
                model.eval()
                ckpt = {
                    'epoch': e + 1,
                    'epoch_psnr_train': epoch_psnr_train,
                    'epoch_psnr_loss' : epoch_psnr_loss,
                    'net_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                }
                torch.save(ckpt, os.path.join(path, 'ckp_epoch_{}.pt'.format(e+1)))
    print("\nTraining is Done.\tTrained model saved at {}".format(path))

def Supervised_train(train_dataloader,model,path,loss_fn,optimizer,epochs,scheduler,radon):
    epoch_psnr_train,epoch_psnr_loss = 0,0
    lists = os.listdir(path)          # list all the files and store to lists
    if lists:                                     # if there is a check points, start training from the newest check point
      lists.sort(key=lambda fn: os.path.getmtime(path + "/" + fn))   # arrange by time 
      file_new = os.path.join(path, lists[-1])                     # get the newest file
      ckpt = torch.load(file_new)
      model.load_state_dict(ckpt['net_state_dict'])
      epoch = ckpt['epoch']
      for e in range(epoch,epochs):
        print(f"Epoch {e+1}\n-------------------------------")
        epoch_psnr_train,epoch_psnr_loss = Supervised_train_loop(train_dataloader, model, loss_fn, optimizer,radon)
        scheduler.step()
        if path is not None and (e + 1) % 100 == 0 :
                model.eval()
                ckpt = {
                    'epoch': e + 1,
                    'epoch_psnr_train': epoch_psnr_train,
                    'epoch_psnr_loss' : epoch_psnr_loss,
                    'net_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                }
                torch.save(ckpt, os.path.join(path, 'ckp_epoch_{}.pt'.format(e+1)))
    else:                                             # if there is no check point, start from 0
      for e in range(epochs):
        print(f"Epoch {e+1}\n-------------------------------")
        epoch_psnr_train,epoch_psnr_loss = Supervised_train_loop(train_dataloader, model, loss_fn, optimizer,radon)
        scheduler.step()
        if path is not None and (e + 1) % 100 == 0 :
                model.eval()
                ckpt = {
                    'epoch': e + 1,
                    'epoch_psnr_train': epoch_psnr_train,
                    'epoch_psnr_loss' : epoch_psnr_loss,
                    'net_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                }
                torch.save(ckpt, os.path.join(path, 'ckp_epoch_{}.pt'.format(e+1)))
    print("\nTraining is Done.\tTrained model saved at {}".format(path))